{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuwVgG3Vbbka"
      },
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Homework 1: Minimal Working Example\n",
        "\n",
        "To make your first submission, follow these steps:\n",
        "1. Create a folder named `[2024-2025] AN2DL/Homework 1` in your Google Drive.\n",
        "2. Upload the `training_set.npz` file to this folder.\n",
        "3. Upload the Jupyter notebook `Homework 1 - Minimal Working Example.ipynb`.\n",
        "4. Load and process the data.\n",
        "5. Implement and train your model.\n",
        "6. Submit the generated `.zip` file to Codabench.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw_-hFm6bjY6"
      },
      "source": [
        "## üåê Connect Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2S4GWr3Uoa8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/AN2DL/Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IqZP5Iblna"
      },
      "source": [
        "## ‚öôÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO6_Ft_8T56A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN_cpHlSboXV"
      },
      "source": [
        "## ‚è≥ Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLaoDaG1V1Yg"
      },
      "outputs": [],
      "source": [
        "data = dict(np.load(\"/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/training_set.npz\"))  # Load the training data here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8C5USLF3anNk"
      },
      "outputs": [],
      "source": [
        "data[\"images\"].shape, data[\"labels\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8ps__dxa4BF"
      },
      "outputs": [],
      "source": [
        "np.unique(data[\"labels\"], return_counts = True) # 8 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC-cpstCct0T"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is already loaded as in the provided code\n",
        "\n",
        "num_images_per_class = 2\n",
        "image_shape = (96, 96, 3)\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 7))\n",
        "j = 0\n",
        "print(data[\"labels\"])\n",
        "for class_label in np.unique(data[\"labels\"]):\n",
        "  class_indices = np.where(data[\"labels\"] == class_label)[0]\n",
        "  selected_indices = np.random.choice(class_indices, size=min(num_images_per_class, len(class_indices)), replace=False)\n",
        "\n",
        "  for i in selected_indices:\n",
        "    image = data[\"images\"][i]\n",
        "    place = axes[j//4][j%4]\n",
        "    place.imshow(image)\n",
        "    place.axis('off')\n",
        "    place.set_title(f\"Class: {class_label}, Index: {i}\")\n",
        "    j+=1\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcXpVgZuO9K_"
      },
      "outputs": [],
      "source": [
        "for class_label in np.unique(data[\"labels\"]):\n",
        "  class_indices = np.where(data[\"labels\"] == class_label)[0]\n",
        "  #print(class_label, data[\"images\"][class_indices].min(), data[\"images\"][class_indices].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4ThtEYtcRxZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from hashlib import md5\n",
        "def find_identical_images(image_dataset, labels):\n",
        "    \"\"\"\n",
        "    Find identical images in a dataset of images stored as NumPy arrays.\n",
        "\n",
        "    Parameters:\n",
        "        image_dataset (list or numpy.ndarray): List or array where each entry is an image in NumPy array format.\n",
        "\n",
        "    Returns:\n",
        "        duplicates (dict): A dictionary where keys are hash values, and values are lists of indices of identical images.\n",
        "    \"\"\"\n",
        "    # Dictionary to store hashes and their corresponding image indices\n",
        "    hash_dict = {}\n",
        "    duplicates = {}\n",
        "\n",
        "    for idx, image in enumerate(image_dataset):\n",
        "        # Flatten the image and compute its hash\n",
        "        image_hash = md5(image.tobytes()).hexdigest()\n",
        "\n",
        "        # Check if the hash already exists\n",
        "        if image_hash in hash_dict:\n",
        "            if image_hash not in duplicates:\n",
        "                duplicates[image_hash] = hash_dict[image_hash] # Start a list with the first duplicate\n",
        "            duplicates[image_hash][\"image_index\"].append(idx)\n",
        "            duplicates[image_hash][\"image_label\"].append(labels[idx][0])\n",
        "        else:\n",
        "            hash_dict[image_hash] = {\"image_index\": [idx], \"image_label\": [labels[idx][0]]}  # Store the index for the hash\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "duplicates = find_identical_images(data[\"images\"], data[\"labels\"])\n",
        "\n",
        "print(duplicates)\n",
        "\n",
        "data[\"image_dubs\"] = np.zeros(data[\"labels\"].shape[0])\n",
        "\n",
        "for hash_value, dubs in duplicates.items():\n",
        "    labels = np.array(dubs[\"image_label\"])\n",
        "    same_label = np.all(labels == labels.flat[0])\n",
        "    if same_label == False:\n",
        "      data[\"image_dubs\"][dubs[\"image_index\"]] = 1\n",
        "    else:\n",
        "      data[\"image_dubs\"][dubs[\"image_index\"][1:]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLEju4HomKXq"
      },
      "outputs": [],
      "source": [
        "tot = 0\n",
        "print(\"Number of duplicate images in each class:\")\n",
        "for class_label in np.unique(data[\"labels\"]):\n",
        "  class_indices = np.where(data[\"labels\"] == class_label)[0]\n",
        "  print(class_label, data[\"image_dubs\"][class_indices].sum())\n",
        "  tot += data[\"image_dubs\"][class_indices].sum()\n",
        "print(tot)\n",
        "\n",
        "filtered_imgs = np.where(data[\"image_dubs\"] == 0)[0]\n",
        "print(f\"len(filtered_imgs)={len(filtered_imgs)}\")\n",
        "new_data = {}\n",
        "new_data[\"images\"] = data[\"images\"][filtered_imgs]\n",
        "new_data[\"labels\"] = data[\"labels\"][filtered_imgs]\n",
        "new_data[\"image_dubs\"] = data[\"image_dubs\"][filtered_imgs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhdZ7G1toDND"
      },
      "outputs": [],
      "source": [
        "find_identical_images(new_data[\"images\"], new_data[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWoqLOv2oJYz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "class_names = [\n",
        "    'Basophil', 'Eosinophil', 'Erythroblast', 'Immature granulocytes',\n",
        "    'Lymphocyte', 'Monocyte', 'Neutrophil', 'Platelet'\n",
        "]\n",
        "labels = new_data[\"labels\"].flatten()\n",
        "# Calculate the total number of samples\n",
        "total_samples = len(labels)\n",
        "class_counts = np.bincount(labels)\n",
        "# Calculate the percentage of each class\n",
        "percentages = (class_counts / total_samples) * 100\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(percentages, labels=class_names, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)\n",
        "plt.title('Percentage of Each Class in the Dataset')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures the pie chart is a circle.\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0YPS_qml5rU"
      },
      "outputs": [],
      "source": [
        "# prompt: find the average value of each channel in images of one class in a center frame of 36 pixels\n",
        "\n",
        "def average_channel_values(images, labels, center_size=54):\n",
        "    \"\"\"\n",
        "    Calculates the average value of each channel for images of a specific class\n",
        "    in a central region of the image.\n",
        "\n",
        "    Args:\n",
        "        images: A NumPy array of images. Shape: (num_images, height, width, channels).\n",
        "        labels: A NumPy array of labels corresponding to the images.\n",
        "        target_class: The class label for which to calculate the averages.\n",
        "        center_size: The size of the central region to consider.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of shape (3,) containing the average values for each channel,\n",
        "        or None if no images of the target class are found.\n",
        "    \"\"\"\n",
        "    grayscale_images = 255 - (\n",
        "        0.2989 * images[..., 0] +  # Red channel\n",
        "        0.5870 * images[..., 1] +  # Green channel\n",
        "        0.1140 * images[..., 2]    # Blue channel\n",
        "    )\n",
        "\n",
        "    avg_gray = 0\n",
        "    for image_orig in grayscale_images:\n",
        "        image = image_orig.copy()\n",
        "        h, w = image.shape\n",
        "        center_y = h // 2\n",
        "        center_x = w // 2\n",
        "        half_size = center_size // 2\n",
        "        y_start = max(0, center_y - half_size)\n",
        "        y_end = min(h, center_y + half_size)\n",
        "        x_start = max(0, center_x - half_size)\n",
        "        x_end = min(w, center_x + half_size)\n",
        "\n",
        "        # image[y_start:y_end, x_start:x_end] = 0\n",
        "        avg_gray += np.mean(image[y_start:y_end, x_start:x_end])\n",
        "\n",
        "    return avg_gray / len(grayscale_images)\n",
        "\n",
        "\n",
        "# Example usage (assuming 'new_data' and 'class_names' are defined as in your previous code):\n",
        "for class_label in np.unique(new_data[\"labels\"]):\n",
        "    filtered_imgs = np.where(new_data[\"labels\"] == class_label)[0]\n",
        "    averages = average_channel_values(new_data[\"images\"][filtered_imgs], new_data[\"labels\"][filtered_imgs])\n",
        "    if averages is not None:\n",
        "        print(f\"Average channel values: {averages}\")\n",
        "    else:\n",
        "        print(f\"No images found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr550n62ycek"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2 as cv\n",
        "\n",
        "\n",
        "masked_data = {}\n",
        "masked_data[\"images\"] = []\n",
        "masked_data[\"labels\"] = []\n",
        "# Assuming 'new_data' is defined as in your provided code\n",
        "for i, image in enumerate(new_data[\"images\"]):\n",
        "    img_orig = new_data[\"images\"][i]\n",
        "    img = cv.cvtColor(255-img_orig, cv.COLOR_BGR2GRAY)\n",
        "    blur = cv.GaussianBlur(img,(5,5),0)\n",
        "    ret3,th3 = cv.threshold(img, 118, 255,cv.THRESH_BINARY)\n",
        "\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "    th3_err = cv.erode(th3, kernel, iterations=1)\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    th3_err = cv.dilate(th3_err, kernel, iterations=5)\n",
        "\n",
        "    masked = cv.bitwise_and(img_orig, img_orig, mask=th3_err)\n",
        "    masked_data[\"images\"].append(masked)\n",
        "    masked_data[\"labels\"].append(new_data[\"labels\"][i])\n",
        "\n",
        "# np.savez(\"/content/drive/MyDrive/AN2DL/Homework 1/masked_data.npz\", images=masked_data[\"images\"],labels=masked_data[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vafhqAiKTRmV"
      },
      "outputs": [],
      "source": [
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "cv.imwrite('output.png', new_data[\"images\"][2000])\n",
        "\n",
        "img_orig = new_data[\"images\"][2000]\n",
        "img = cv.cvtColor(255-img_orig, cv.COLOR_BGR2GRAY)\n",
        "# Otsu's thresholding after Gaussian filtering\n",
        "\n",
        "blur = cv.GaussianBlur(img,(5,5),0)\n",
        "ret3,th3 = cv.threshold(img, 118, 255,cv.THRESH_BINARY)\n",
        "\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "th3_err = cv.erode(th3, kernel, iterations=1)\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "th3_err = cv.dilate(th3_err, kernel, iterations=5)\n",
        "\n",
        "masked = cv.bitwise_and(img_orig, img_orig, mask=th3_err)\n",
        "\n",
        "plt.subplot(1,4,1),plt.imshow(img_orig)\n",
        "plt.subplot(1,4,2),plt.imshow(img,'gray')\n",
        "plt.subplot(1,4,3),plt.imshow(th3,'gray')\n",
        "plt.subplot(1,4,4),plt.imshow(masked)\n",
        "masked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Si1W_x-PUJ-h"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "colors = (\"red\", \"green\", \"blue\")\n",
        "bins = np.arange(256)\n",
        "\n",
        "class_names = [\n",
        "    'Basophil', 'Eosinophil', 'Erythroblast', 'Immature granulocytes',\n",
        "    'Lymphocyte', 'Monocyte', 'Neutrophil', 'Platelet'\n",
        "]\n",
        "class_labels = np.array([i for i in range(8)]).astype(np.uint8)\n",
        "\n",
        "# create the histogram plot, with three lines, one for\n",
        "# each color\n",
        "plt.rc('axes', titlesize=8)  # Set the font size of subplot titles\n",
        "plt.rc('axes', labelsize=8)\n",
        "fig, axes = plt.subplots(8, 6, figsize=(14, 8))\n",
        "\n",
        "column_labels = ['Example image', 'Average image', 'Hist for red', 'Hist for green', 'Hist for blue', 'Hist for gray']\n",
        "for ax, label in zip(axes[0], column_labels):\n",
        "    ax.set_title(label, fontsize=8)\n",
        "\n",
        "row_labels = class_names\n",
        "for ax, label in zip(axes[:, 0], row_labels):  # First column only\n",
        "    ax.set_ylabel(label, fontsize=8, labelpad=10, rotation=45, ha='right', va='center')\n",
        "\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for label in class_labels:\n",
        "\n",
        "  filtered_imgs = np.where(masked_data[\"labels\"] == label)[0]\n",
        "  images = np.array(masked_data[\"images\"])[filtered_imgs]\n",
        "  num_images = len(images)\n",
        "  grayscale_images = 255 - (\n",
        "    0.2989 * images[..., 0] +  # Red channel\n",
        "    0.5870 * images[..., 1] +  # Green channel\n",
        "    0.1140 * images[..., 2]    # Blue channel\n",
        "  )\n",
        "\n",
        "  axes[label*6].imshow(images[0])\n",
        "  # axes[label*6].axis(\"off\")\n",
        "  axes[label*6+1].imshow(np.mean(images, axis=0).astype(np.uint8))\n",
        "  # axes[label*6+1].axis(\"off\")\n",
        "\n",
        "  for channel_id, color in enumerate(colors):\n",
        "    hist = np.zeros(len(bins), dtype=np.float32)\n",
        "\n",
        "    for img in images:\n",
        "        # Compute histograms for each channel\n",
        "        histogram, bin_edges = np.histogram(\n",
        "        img[:, :, channel_id], bins=256, range=(0, 256)\n",
        "    )\n",
        "        hist+=histogram\n",
        "\n",
        "    # Average the histograms\n",
        "    hist /= num_images\n",
        "    axes[6*label+2+channel_id].plot(bins[1:], hist[1:], color=color)\n",
        "\n",
        "  hist = np.zeros(len(bins), dtype=np.float32)\n",
        "  for img in grayscale_images:\n",
        "        histogram, bin_edges = np.histogram(\n",
        "        img, bins=256, range=(0, 256)\n",
        "    )\n",
        "        hist+=histogram\n",
        "  hist /= num_images\n",
        "  axes[6*label+5].plot(bins[:-1], hist[:-1], color=\"k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSliIxBvbs2Q"
      },
      "source": [
        "## Split the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "seed = 42\n",
        "#X = (np.array(new_data['images'])/255).astype('float32')\n",
        "X = (np.array(masked_data[\"images\"])/255).astype('float32')\n",
        "#y = np.array(labels)\n",
        "y = np.array(masked_data[\"labels\"])\n",
        "y = tfk.utils.to_categorical(y)\n",
        "print(f\"shape of x: {X.shape}\")\n",
        "print(f\"shape of y: {y.shape}\")\n",
        "print(f\"max value in X: {X.max()}\")\n",
        "\n",
        "# Split data into train_val and test sets\n",
        "#X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, random_state=seed, test_size=0.1, stratify=y)\n",
        "\n",
        "# Further split train_val into train and validation sets\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=0.1, stratify=y_train_val)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.1, stratify=y)\n",
        "print(\"Training Data Shape:\", X_train.shape)\n",
        "print(\"Training Label Shape:\", y_train.shape)\n",
        "print(\"Validation Data Shape:\", X_val.shape)\n",
        "print(\"Validation Label Shape:\", y_val.shape)\n",
        "#print(\"Test Data Shape:\", X_test.shape)\n",
        "#print(\"Test Label Shape:\", y_test.shape)"
      ],
      "metadata": {
        "id": "7Oq_7PSwibmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add augmentation"
      ],
      "metadata": {
        "id": "vI0Lae77MI04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the function which will be applied to all images\n",
        "augmentation = tfk.Sequential([\n",
        "    tfkl.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tfkl.RandomRotation(0.3),\n",
        "    #tfkl.RandomTranslation(0.05,0.05),\n",
        "    tfkl.RandomZoom(0.1),\n",
        "    tfkl.RandomBrightness(0.2, value_range=(0,1)),\n",
        "    tfkl.RandomContrast(0.25),\n",
        "], name='Augmentation')\n",
        "\n",
        "#Plot images before augmentation\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(X_train[0])\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(X_train[1])\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(X_train[2])\n",
        "\n",
        "#We want to normalize so each category has the same amount of images\n",
        "y_train_labels = np.argmax(y_train, axis=1) #from one-hot to label encoding\n",
        "class_counts = np.bincount(y_train_labels)\n",
        "print(f\"class counts before augmentation {class_counts}\")\n",
        "#add images until each category has 3000\n",
        "images_to_append = np.empty((13244,96,96,3))\n",
        "labels_to_append = np.empty((13244,8))\n",
        "append_at_index = 0\n",
        "index = 0\n",
        "while append_at_index < len(images_to_append):\n",
        "  #index = index_app % len(X_train)\n",
        "  image = X_train[index]\n",
        "  if class_counts[y_train_labels[index]] < 3000:\n",
        "    images_to_append[append_at_index] = image\n",
        "    labels_to_append[append_at_index] = y_train[index]\n",
        "\n",
        "    append_at_index += 1\n",
        "    class_counts[y_train_labels[index]] += 1\n",
        "  index += 1\n",
        "  if index >= len(X_train):\n",
        "    index = index %len(X_train)\n",
        "\n",
        "\n",
        "#augmentation done in many steps because otherwise colab runs out of ram\n",
        "steps = 40\n",
        "for i in range(steps):\n",
        "  start_ind = (i)*len(X_train)//steps\n",
        "  end_ind = (i+1)*len(X_train)//steps\n",
        "  X_train[start_ind:end_ind] = augmentation(X_train[start_ind:end_ind])\n",
        "for i in range(steps):\n",
        "  start_ind = (i)*len(images_to_append)//steps\n",
        "  end_ind = (i+1)*len(images_to_append)//steps\n",
        "  images_to_append[start_ind:end_ind] = augmentation(images_to_append[start_ind:end_ind])\n",
        "\n",
        "print(f\"class counts after augmentation {class_counts}\")\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(X_train[0])\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(X_train[1])\n",
        "plt.subplot(2, 3, 6)\n",
        "plt.imshow(X_train[2])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 3, 2).set_title(\"Original images\", fontsize=14)#, labelpad=10, rotation=45, ha='right', va='center')\n",
        "plt.subplot(2, 3, 5).set_title(\"Augmented images\", fontsize=14)#, labelpad=10, rotation=45, ha='right', va='center')\n",
        "\n",
        "#.set_title(\"Title for first plot\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#this part is to save augmented_images.npz as a file\n",
        "\"\"\"try:\n",
        "  np.savez_compressed(\"/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/augmented_images.npz\", X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val)\n",
        "  print(\"successful file save\")\n",
        "except Exception as e:\n",
        "  print(f\"an error occured: {e}\")\"\"\"\n"
      ],
      "metadata": {
        "id": "cgNSbG6FMDwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape for the model\n",
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "# Output shape for the model\n",
        "output_shape = y_train.shape[1]\n",
        "\n",
        "print(\"Input Shape:\", input_shape)\n",
        "print(\"Output Shape:\", output_shape)\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 20\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 128\n",
        "\n",
        "# Learning rate: step size for updating the model's weights\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Print the defined parameters\n",
        "print(\"Epochs:\", epochs)\n",
        "print(\"Batch Size:\", batch_size)\n",
        "print(\"Learning Rare:\", learning_rate)"
      ],
      "metadata": {
        "id": "g1N-orWUDEb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG19 PRETRAINED MODEL\n"
      ],
      "metadata": {
        "id": "4rFsteHQpVT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2LCBUzfHRN02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape=input_shape,\n",
        "    output_shape=output_shape,\n",
        "    learning_rate=learning_rate,\n",
        "    seed=seed\n",
        "):\n",
        "  inputs = tfkl.Input(shape=input_shape)\n",
        "  output = inputs\n",
        "\n",
        "  #pretrained_model = tf.keras.applications.VGG19(include_top=False, input_shape=input_shape, weights='imagenet')\n",
        "  pretrained_model = tf.keras.applications.VGG19(include_top=False, input_shape=input_shape, weights='imagenet')\n",
        "  output = pretrained_model(output)\n",
        "\n",
        "  output = tfkl.Flatten(name='flatten')(output)\n",
        "\n",
        "  output = tfkl.Dense(output_shape, activation='softmax', name='last_dense')(output)\n",
        "\n",
        "  pretrained_model.trainable = False\n",
        "  # Connect input and output through the Model class\n",
        "  model = tfk.Model(inputs=inputs, outputs=output, name='CNN')\n",
        "\n",
        "  # Compile the model\n",
        "  loss = tfk.losses.CategoricalCrossentropy()\n",
        "  optimizer = tfk.optimizers.Adam(learning_rate) #switch from adam?\n",
        "  metrics = ['accuracy']\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "  # Return the model\n",
        "  return model, pretrained_model\n",
        "\n",
        "# Define the patience value for early stopping\n",
        "patience = 10\n",
        "\n",
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Store the callback in a list\n",
        "callbacks = [early_stopping]"
      ],
      "metadata": {
        "id": "1cMZl1u1pU19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with specified input and output shapes\n",
        "#model, pretrained_model = build_model()\n",
        "model, pretrained_model = build_model()\n",
        "\n",
        "# Plot the model architecture\n",
        "#tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)\n",
        "tfk.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)"
      ],
      "metadata": {
        "id": "PD9ppLU2QcFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=20,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks\n",
        ").history\n",
        "\n",
        "#Fine tuning\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "learning_rate = learning_rate / 20\n",
        "\n",
        "\n",
        "pretrained_model.trainable = False\n",
        "for layer in pretrained_model.layers[len(pretrained_model.layers)-10:]:\n",
        "  layer.trainable = True\n",
        "\n",
        "# It's important to recompile your model after you make any changes\n",
        "# to the `trainable` attribute of any inner layer, so that your changes\n",
        "# are take into account\n",
        "model.compile(optimizer=tfk.optimizers.Adam(learning_rate/10),  # Very low learning rate\n",
        "              loss=tfk.losses.CategoricalCrossentropy(),#tfk.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train end-to-end. Be careful to stop before you overfit!\n",
        "history2 = model.fit(\n",
        "  x=X_train,\n",
        "  y=y_train,\n",
        "  batch_size=batch_size,\n",
        "  epochs=10,\n",
        "  validation_data=(X_val, y_val),\n",
        "  callbacks=callbacks\n",
        ").history\n",
        "#history = dict(history1) #full history\n",
        "#history.update(history2)\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "#final_val_accuracy = round(max(history1['val_accuracy'])* 100, 2)\n",
        "#max_binary_accuracy = round(max(history2['binary_accuracy'])* 100, 2)\n",
        "#print(f'Max binary accuracy: {max_binary_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "#model_filename = 'CIFAR10_CNN_'+str(final_val_accuracy)+'.keras'\n",
        "#model.save(model_filename)"
      ],
      "metadata": {
        "id": "hQv6CD9qQnHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pretrained_model.layers)\n",
        "for layer in pretrained_model.layers[len(pretrained_model.layers)-10:]:\n",
        "  print(layer)"
      ],
      "metadata": {
        "id": "n5JG86RNvBUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Plot training and validation loss\n",
        "#plt.figure()\n",
        "plt.plot(history1['loss']+history2['loss'], label='Training loss', alpha=.8)\n",
        "plt.plot(history1['val_loss']+history2['val_loss'], label='Validation loss', alpha=.8)\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(15, 2))\n",
        "plt.plot(history1['accuracy'] + history2[\"accuracy\"], label='Training accuracy', alpha=.8)\n",
        "plt.plot(history1['val_accuracy'] + history2[\"val_accuracy\"], label='Validation accuracy', alpha=.8)\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "colors = (\"red\", \"green\", \"blue\")\n",
        "bins = np.arange(256)\n",
        "\n",
        "plt.show()\n",
        "#model.save(\"/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/weights.keras\")"
      ],
      "metadata": {
        "id": "tIMw2F0xRU2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "#model = tfk.models.load_model('CIFAR10_CNN_92.75.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "#model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "#tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "F-OuwCEXRXt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels for the entire test set\n",
        "predictions = model.predict(X_val, verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)"
      ],
      "metadata": {
        "id": "iiajKULJRbnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "# Convert predictions to class labels\n",
        "pred_classes = np.argmax(predictions, axis=-1)\n",
        "\n",
        "# Extract ground truth classes\n",
        "true_classes = np.argmax(y_val, axis=-1)\n",
        "print(f\"first predictions: {pred_classes[:5]}\")\n",
        "print(f\"first true classes: {true_classes[:5]}\")\n",
        "\n",
        "# Calculate and display test set accuracy\n",
        "accuracy = accuracy_score(true_classes, pred_classes)\n",
        "print(f'Accuracy score over the test set: {round(accuracy, 4)}')\n",
        "\n",
        "# Calculate and display test set precision\n",
        "precision = precision_score(true_classes, pred_classes, average='weighted')\n",
        "print(f'Precision score over the test set: {round(precision, 4)}')\n",
        "\n",
        "# Calculate and display test set recall\n",
        "recall = recall_score(true_classes, pred_classes, average='weighted')\n",
        "print(f'Recall score over the test set: {round(recall, 4)}')\n",
        "\n",
        "# Calculate and display test set F1 score\n",
        "f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
        "print(f'F1 score over the test set: {round(f1, 4)}')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(true_classes, pred_classes)\n",
        "\n",
        "# Combine numbers and percentages into a single string for annotation\n",
        "annot = np.array([f\"{num}\" for num in cm.flatten()]).reshape(cm.shape)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "labels = [\n",
        "    'Basophil', 'Eosinophil', 'Erythroblast', 'Immature granulocytes',\n",
        "    'Lymphocyte', 'Monocyte', 'Neutrophil', 'Platelet'\n",
        "]\n",
        "sns.heatmap(cm.T, annot=annot, fmt='', xticklabels=list(labels), yticklabels=list(labels), cmap='Blues')\n",
        "plt.xlabel('True labels')\n",
        "plt.ylabel('Predicted labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oo1Yk_vmRcFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox9jqYyyUJo0"
      },
      "outputs": [],
      "source": [
        "model.save('weights.keras')\n",
        "# del model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-fold validation"
      ],
      "metadata": {
        "id": "IrJxebfTUBoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "def train_and_evaluate(data, labels, model_name, learning_rates, optimizers, k=5, epochs=30, batch_size=128):\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    best_combination = None\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for optimizer_name in optimizers:\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Evaluating optimizer={optimizer_name}, learning_rate={lr}\")\n",
        "\n",
        "            train_losses, val_losses, train_accuracies, val_accuracies, test_accuracies, test_f1 = [], [], [], [], [], []\n",
        "\n",
        "            for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
        "                print(f\"  Fold {fold + 1}/{k}\")\n",
        "\n",
        "                # Split the data into training and validation sets\n",
        "                train_data, val_data = data[train_idx], data[val_idx]\n",
        "                train_labels, val_labels = labels[train_idx], labels[val_idx]\n",
        "\n",
        "                train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
        "                val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
        "\n",
        "                # Shuffle, batch, and prefetch for training\n",
        "                train_dataset = train_dataset.shuffle(buffer_size=1000).batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "                val_dataset = val_dataset.batch(128).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "                # Create and compile the model\n",
        "                if optimizer_name == 'adam':\n",
        "                    optimizer = Adam(learning_rate=lr)\n",
        "                elif optimizer_name == 'sgd':\n",
        "                    optimizer = SGD(learning_rate=lr)\n",
        "                elif optimizer_name == 'rmsprop':\n",
        "                    optimizer = RMSprop(learning_rate=lr)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "                patience = 5\n",
        "\n",
        "                # Create an EarlyStopping callback\n",
        "                early_stopping = tfk.callbacks.EarlyStopping(\n",
        "                    monitor='val_loss',\n",
        "                    mode='max',\n",
        "                    patience=patience,\n",
        "                    restore_best_weights=True\n",
        "                )\n",
        "\n",
        "                # Store the callback in a list\n",
        "                callbacks = [early_stopping]\n",
        "                loss = tfk.losses.CategoricalCrossentropy()\n",
        "\n",
        "                model = build_model()\n",
        "                model.compile(optimizer=optimizer,\n",
        "                              loss=loss,\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "                # Train the model\n",
        "                history = model.fit(train_dataset,\n",
        "                                    validation_data=val_dataset,\n",
        "                                    callbacks=callbacks,\n",
        "                                    epochs=epochs,\n",
        "                                    batch_size=batch_size)\n",
        "\n",
        "                # Record metrics\n",
        "                train_losses.append(history.history['loss'])\n",
        "                val_losses.append(history.history['val_loss'])\n",
        "                train_accuracies.append(history.history['accuracy'])\n",
        "                val_accuracies.append(history.history['val_accuracy'])\n",
        "\n",
        "                predictions = model.predict(X_test, verbose=0)\n",
        "                pred_classes = np.argmax(predictions, axis=-1)\n",
        "                true_classes = np.argmax(y_test, axis=-1)\n",
        "                test_accuracy = accuracy_score(true_classes, pred_classes)\n",
        "                f1 = f1_score(true_classes, pred_classes, average='weighted')\n",
        "\n",
        "                test_accuracies.append(test_accuracy)\n",
        "                test_f1.append(f1)\n",
        "\n",
        "                final_val_accuracy = round(max(history.history['val_accuracy'])* 100, 2)\n",
        "\n",
        "                model_filename = f'{model_name}_lr_{lr}_opt_{optimizer_name}_{final_val_accuracy}_fold_{fold}.keras'\n",
        "                model.save(model_filename)\n",
        "\n",
        "\n",
        "\n",
        "            # Compute average metrics over all folds\n",
        "            avg_train_loss = np.mean([loss[-1] for loss in train_losses])\n",
        "            avg_val_loss = np.mean([loss[-1] for loss in val_losses])\n",
        "            avg_train_accuracy = np.mean([acc[-1] for acc in train_accuracies])\n",
        "            avg_val_accuracy = np.mean([acc[-1] for acc in val_accuracies])\n",
        "            std_val_accuracy = np.std([acc[-1] for acc in val_accuracies])\n",
        "\n",
        "            avg_test_accuracy = np.mean([acc[-1] for acc in test_accuracies])\n",
        "            std_test_accuracy = np.std([acc[-1] for acc in test_accuracies])\n",
        "            avg_test_f1 = np.mean([f1[-1] for f1 in test_f1])\n",
        "            std_test_f1 = np.std([f1[-1] for f1 in test_f1])\n",
        "\n",
        "            print(f\"Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
        "            print(f\"Avg Train Accuracy: {avg_train_accuracy:.4f}, Avg Val Accuracy: {avg_val_accuracy:.4f}, , Std Val Accuracy: {std_val_accuracy:.4f}\")\n",
        "            print(f\"Avg Test Accuracy: {avg_test_accuracy:.4f}, Std Test Accuracy: {std_test_accuracy:.4f}, Avg Test F1: {avg_test_f1:.4f},  Std Test F1: {std_test_f1:.4f}\")\n",
        "\n",
        "            results.append({\n",
        "                'optimizer': optimizer_name,\n",
        "                'learning_rate': lr,\n",
        "                'avg_val_loss': avg_val_loss,\n",
        "                'avg_val_accuracy': avg_val_accuracy,\n",
        "                'std_val_accuracy': std_val_accuracy,\n",
        "                'avg_test_accuracy': avg_test_accuracy,\n",
        "                'std_test_accuracy': std_test_accuracy,\n",
        "                'avg_test_f1': avg_test_f1,\n",
        "                'std_test_f1': std_test_f1\n",
        "            })\n",
        "\n",
        "            # Update best combination if validation loss improves\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                best_combination = {'optimizer': optimizer_name, 'learning_rate': lr}\n",
        "\n",
        "            # Plot training/validation loss and accuracy for the combination\n",
        "            plt.figure(figsize=(12, 5))\n",
        "\n",
        "            # Loss plot\n",
        "            plt.subplot(1, 2, 1)\n",
        "            for i in range(k):\n",
        "                plt.plot(train_losses[i], label=f'Train Loss (Fold {i+1})', alpha=0.7)\n",
        "                plt.plot(val_losses[i], label=f'Val Loss (Fold {i+1})', alpha=0.7)\n",
        "            plt.title(f'Loss - Optimizer {model_name}: {optimizer_name}, LR: {lr}')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "\n",
        "            # Accuracy plot\n",
        "            plt.subplot(1, 2, 2)\n",
        "            for i in range(k):\n",
        "                plt.plot(train_accuracies[i], label=f'Train Accuracy (Fold {i+1})', alpha=0.7)\n",
        "                plt.plot(val_accuracies[i], label=f'Val Accuracy (Fold {i+1})', alpha=0.7)\n",
        "            plt.title(f'Accuracy - Optimizer {model_name}: {optimizer_name}, LR: {lr}')\n",
        "            plt.xlabel('Epochs')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    print(\"\\nBest Combination:\")\n",
        "    print(best_combination)\n",
        "\n",
        "# Example dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define learning rates and optimizers to evaluate\n",
        "learning_rates = [0.01, 0.001, 0.0005]\n",
        "optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "\n",
        "# Perform cross-validation and parameter tuning\n",
        "train_and_evaluate(X_val, y_val, \"Custom\", learning_rates, optimizers, k=5, epochs=10, batch_size=128)\n"
      ],
      "metadata": {
        "id": "tFO77mZfUAB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNp6pUZuddqC"
      },
      "source": [
        "## üìä Prepare Your Submission\n",
        "\n",
        "To prepare your submission, create a `.zip` file that includes all the necessary code to run your model. It **must** include a `model.py` file with the following class:\n",
        "\n",
        "```python\n",
        "# file: model.py\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\"\n",
        "```\n",
        "\n",
        "The next cell shows an example implementation of the `model.py` file, which includes loading model weights from the `weights.keras` file and conducting predictions on provided input data. The `.zip` file is created and downloaded in the last notebook cell.\n",
        "\n",
        "‚ùó Feel free to modify the method implementations to better fit your specific requirements, but please ensure that the class name and method interfaces remain unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKT4h-9xYwiT"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "import cv2 as cv\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the internal state of the model. Note that the __init__\n",
        "        method cannot accept any arguments.\n",
        "\n",
        "        The following is an example loading the weights of a pre-trained\n",
        "        model.\n",
        "        \"\"\"\n",
        "        self.neural_network = tfk.models.load_model('weights.keras')\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
        "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
        "        array of shape (n_samples,). Therefore, outputs must not be one-hot\n",
        "        encoded.\"\"\"\n",
        "\n",
        "        X = X/255\n",
        "\n",
        "        preds = self.neural_network.predict(X)\n",
        "\n",
        "        if len(preds.shape) == 2:\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing model.py"
      ],
      "metadata": {
        "id": "uWf_5dwwDmms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import model as model_file\n",
        "#following two lines are necessary to update model_file after changing it due...\n",
        "#to jupyter notebook architecture\n",
        "import importlib\n",
        "importlib.reload(model_file)\n",
        "\n",
        "final_model = model_file.Model()\n",
        "X_test_corrected = X_val*255 #these are the same as the actual test set in range\n",
        "final_pred = final_model.predict(X_test_corrected)\n",
        "print(f\"first predictions: {final_pred[:25]}\")\n",
        "print(f\"first true classes: {true_classes[:25]}\")\n",
        "\n",
        "print(f\"shape of the predictions: {final_pred.shape}\")\n",
        "final_accuracy = accuracy_score(true_classes, final_pred)\n",
        "print(f\"final accuracy: {final_accuracy}\")"
      ],
      "metadata": {
        "id": "FlhmNY4VDmCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s18kX1uDconq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
        "\n",
        "# Add files to the zip command if needed\n",
        "!zip {filename} model.py weights.keras\n",
        "\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}